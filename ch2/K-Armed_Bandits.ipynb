{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tnrange\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    \n",
    "    def __init__(self, k=10, mean=0.0, sigma=1.0):\n",
    "        self.k = k\n",
    "        self.mean = mean\n",
    "        self.sigma = sigma\n",
    "        self.k_arms = np.arange(self.k)\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def reward(self, action):\n",
    "        # Produce a reward using N(µ=true reward, σ^2=1)\n",
    "        # Old: reward = np.random.randn() + self.q_true[action]\n",
    "        return np.random.normal(self.q_star[action], scale=1.0)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.q_star = np.random.normal(self.mean, self.sigma, self.k)\n",
    "        self.q_best = max(self.q_star)\n",
    "        self.best_action = np.random.choice(np.flatnonzero(self.q_star == self.q_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStationaryEnvironment(Environment):\n",
    "    \n",
    "    \n",
    "    def __init__(self, k=10, mean=0.0, sigma=1.0, rand_walk=0.01):\n",
    "        super().__init__(k, mean, sigma)\n",
    "        self.rand_walk = rand_walk\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        \n",
    "        \n",
    "    def reward(self, action):\n",
    "        self.q_star += np.random.normal(0.0, scale=self.rand_walk, size=self.k)\n",
    "        self.q_best = max(self.q_star)\n",
    "        self.best_action = np.random.choice(np.flatnonzero(self.q_star == self.q_best))\n",
    "        return super().reward(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "    \n",
    "    \n",
    "    def __init__(self, k=10, epsilon=0.0, initial=0.0, alpha=None):\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.initial = initial\n",
    "        self.action_space = np.arange(self.k)\n",
    "        self.alpha = alpha\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.alpha:\n",
    "            return f\"EpsilonGreedy | ε = {self.epsilon:.02f}, α = {self.alpha:0.02f}\"\n",
    "        return f\"EpsilonGreedy | ε = {self.epsilon:.02f}\"\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.q_est = np.zeros(self.k) + self.initial\n",
    "        self.action_count = np.zeros(self.k)\n",
    "        \n",
    "        \n",
    "    def act(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "\n",
    "        q_best = max(self.q_est)\n",
    "        return np.random.choice(np.flatnonzero(self.q_est == q_best))\n",
    "    \n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        self.action_count[action] += 1\n",
    "        if self.alpha:\n",
    "            # Use constant step size updates\n",
    "            self.q_est[action] += self.alpha * (reward - self.q_est[action])\n",
    "        else:\n",
    "            # Use sample averaging\n",
    "            self.q_est[action] += (reward - self.q_est[action]) / self.action_count[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent(EpsilonGreedyAgent):\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, c):\n",
    "        super().__init__(k)\n",
    "        self.steps = 0\n",
    "        self.c = c\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"UCBAgent | c = {self.c}\"\n",
    "    \n",
    "    \n",
    "    def act(self):\n",
    "        # Add small number .0001 to prevent empty action count\n",
    "        ucb_est = self.q_est + self.c * np.sqrt(np.log(self.steps + 1) / (self.action_count + 1e-5))\n",
    "        q_best = max(ucb_est)\n",
    "        return np.random.choice(np.flatnonzero(ucb_est == q_best))\n",
    "    \n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        self.steps += 1\n",
    "        super().update(action, reward)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "tp = ThreadPoolExecutor(10)\n",
    "\n",
    "\n",
    "def threaded(fn):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return tp.submit(fn, *args, **kwargs)\n",
    "    return wrapper\n",
    "    \n",
    "\n",
    "class Simulate:\n",
    "    \n",
    "    \n",
    "    def __init__(self, env, agents, runs=2000, steps=1000):\n",
    "        self.env = env\n",
    "        self.agents = agents\n",
    "        self.runs = runs\n",
    "        self.steps = steps\n",
    "        self.rewards = np.zeros((len(agents), runs, steps))\n",
    "        self.best_action_counts = np.zeros(self.rewards.shape)\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "        \n",
    "    @threaded\n",
    "    def simulate_agent(agent, runs, steps):\n",
    "        \"\"\" WIP\n",
    "        \n",
    "            Parallelize each agent as a parent thread to execute \n",
    "            runs and aggregate results\n",
    "        \n",
    "        \"\"\"\n",
    "        rewards = np.zeros(runs, steps)\n",
    "        best_action_counts = np.zeros(runs, steps)\n",
    "        \n",
    "        for run in tnrange(runs):\n",
    "            self.simulate_run(agent, steps)\n",
    "                    \n",
    "        return rewards, best_action_counts\n",
    "\n",
    "    \n",
    "    @threaded\n",
    "    def simulate_run(agent, steps):\n",
    "        \"\"\" WIP - Parallelize each run\n",
    "        \"\"\"\n",
    "        env = deepcopy(self.env)\n",
    "        agent = deepcopy(agent)\n",
    "        rewards = np.zeros(steps)\n",
    "        best_action_counts = np.zeros(steps)\n",
    "        \n",
    "        env.reset()\n",
    "        agent.reset()\n",
    "        for step in range(steps):\n",
    "            action = agent.act()\n",
    "            reward = env.reward(action)\n",
    "            agent.update(action, reward)\n",
    "            rewards[step] += reward\n",
    "            if action == env.best_action:\n",
    "                best_action_counts[step] += 1\n",
    "                    \n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\" Legacy Entrypoint - Naive parallelism \"\"\"\n",
    "        def work(env, agent, runs, steps, idx):\n",
    "            for run in tnrange(runs):\n",
    "                env.reset()\n",
    "                agent.reset()\n",
    "                for step in range(steps):\n",
    "                    action = agent.act()\n",
    "                    reward = env.reward(action)\n",
    "                    agent.update(action, reward)\n",
    "\n",
    "                    #with self._lock:\n",
    "                    self.rewards[idx, run, step] += reward\n",
    "                    if action == env.best_action:\n",
    "                        self.best_action_counts[idx, run, step] += 1\n",
    "\n",
    "\n",
    "        threads = list()\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            t = threading.Thread(target=work, args=(deepcopy(self.env), agent, self.runs, self.steps, i))\n",
    "            threads.append(t)\n",
    "            t.start()\n",
    "\n",
    "\n",
    "        for idx, thread in enumerate(threads):\n",
    "            thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_agent_performance(simulator):\n",
    "    reward_mean = np.mean(simulator.rewards, axis=1)\n",
    "    best_action_count_mean = np.mean(simulator.best_action_counts, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12.5, 10))\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    for i, agent in enumerate(simulator.agents):\n",
    "        ax1.plot(reward_mean[i], label=str(agent))\n",
    "        ax2.plot(best_action_count_mean[i], label=str(agent))\n",
    "    \n",
    "    ax1.set_xlabel('Steps')\n",
    "    ax1.set_ylabel('Average Reward')\n",
    "    ax1.legend()\n",
    "        \n",
    "    ax2.set_xlabel('Steps')\n",
    "    ax2.set_ylabel('% Optimal Action')\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 10-Armed Testbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violin Plot\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Reward Distribution')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12.5, 5))\n",
    "plt.violinplot(dataset=np.random.randn(2000, 10) + np.random.randn(10), showmeans=True)\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Reward Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2.2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "agents = [ EpsilonGreedyAgent(env.k, epsilon) for epsilon in [0, 0.01, 0.1] ]\n",
    "\n",
    "sim = Simulate(env, agents)\n",
    "sim.run()\n",
    "\n",
    "plot_agent_performance(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking a Nonstationary Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.4\n",
    "---\n",
    "If the step-size parameters, $ \\alpha_n $, are not constant, then the estimate $ Q_n $ is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?  \n",
    "\n",
    "\\begin{equation*}\n",
    "Q_{n+1}=(1 - \\alpha)^n Q_1 + \\sum_{i=1}^{n}\\alpha(1-\\alpha)^{n-i}R_i \\tag{2.6}\n",
    "\\end{equation*}\n",
    "\n",
    "### Solution:\n",
    "---\n",
    "Let $ \\alpha_{n}(a) $ vary with time:\n",
    "\n",
    "\\begin{align*}\n",
    "Q_{n+1}\n",
    "&= Q_n + \\alpha_n[R_n - Q_n] \\\\\n",
    "&= (1 - \\alpha_n)Q_n + \\alpha_nR_n \\\\\n",
    "&= (1 - \\alpha_n)(\\alpha_{n-1}R_{n-1} + (1 - \\alpha_{n-1})Q_{n-1}) + \\alpha_nR_n \\\\\n",
    "&= (1 - \\alpha_n)\\alpha_{n-1}R_{n-1} + (1 - \\alpha_n)(1 - \\alpha_{n-1})Q_{n-1} + \\alpha_nR_n \\\\\n",
    "&= (1 - \\alpha_n)\\alpha_{n-1}R_{n-1} + (1 - \\alpha_n)^2\\alpha_nR_{n-2} + ... + (1 - \\alpha_n)^{n-1}\\alpha_nR_1  + (1 - \\alpha_n)^nQ_1 + \\alpha_nR_n \\\\\n",
    "&= Q_1\\prod_{i=1}^n(1 - \\alpha_i) + \\sum_{i=1}^n(\\alpha_iR_i\\prod_{j=i}^n(1 - \\alpha_j))\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.5\n",
    "---\n",
    "Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the $ q_{*}(a) $ start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the $ q_{*}(a) $ on each step).\n",
    "\n",
    "Prepare plots like Figure 2.2 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, $ \\alpha=0.1 $. Use $ \\epsilon=0.1 $ and longer runs, say of 10,000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NonStationaryEnvironment()\n",
    "agents = [EpsilonGreedyAgent(env.k, epsilon=0.1), EpsilonGreedyAgent(env.k, epsilon=0.1, alpha=0.1)]\n",
    "\n",
    "sim = Simulate(env, agents, steps=10000)\n",
    "sim.run()\n",
    "\n",
    "plot_agent_performance(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "agents = [EpsilonGreedyAgent(env.k, alpha=0.1, initial=5), EpsilonGreedyAgent(env.k, epsilon=0.1, alpha=0.1)]\n",
    "\n",
    "sim = Simulate(env, agents)\n",
    "sim.run()\n",
    "\n",
    "plot_agent_performance(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.6: Mysterious Spikes\n",
    "---\n",
    "The results shown in Fig 2.3 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?\n",
    "\n",
    "### Solution:\n",
    "\n",
    "With an initial estimate of 5 and a stationary reward distribution given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_{*}(a) \\sim \\mathcal{N}(\\mu=0, \\sigma^{2}=1)\n",
    "\\end{equation*}\n",
    "\n",
    "During the first k steps of each run, the bandit will sample each available step at least once as there effectively no chance the sampled reward $ R_{n}(a) \\sim \\mathcal{N}(\\mu=Q_{*}(a), \\sigma^{2}=1) $ will be >= 5 for n <= k. As each sampled action during step $ \\{ n | n <= k \\} $ adjusts the expected reward downwards toward the true reward, after all estimates have been updated by each action on step $ n = k + 1 $ having been selected, the bandit has a strong probability of sampling the optimal action which explains the initial spike at $ step=k + 1=11 $. \n",
    "\n",
    "There is overlap in the reward distribution $ Q_{*}(a) $ so it is highly probable that the optimal action will sample lower than other actions and the greedy algorithm will act on them subsequently in early steps before estimates have annealed. As the sample size grows the expected return from the optimal action will more consistently be greater than the other actions and will be selected correctly more often in greater time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.7: Unbiased Constant Step Size Trick\n",
    "---\n",
    "Throughout most of the chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do. However, sample averages are not a completely satisfactory solution because they may perform poorly on nonstationary problems. Is it possible to avoid the bias of constant step sizes while on non-stationary problems? One way is to use a step size of:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta_n \\doteq \\frac{\\alpha}{\\bar{o}_n}\n",
    "\\end{equation*}\n",
    "\n",
    "to process the nth reward for a particular action, where $ \\alpha > 0 $ is a conventional constant step size, and $ \\bar{o}_n $ is a trace of one that starts at 0:\n",
    "\n",
    "$ \\bar{o}_n \\doteq \\bar{o}_{n-1} + \\alpha(1 - \\bar{o}_{n-1}) $ , for $ n \\geq 0 $ , with $ \\bar{o}_0 \\doteq 0 $.\n",
    "\n",
    "Carry out an analysis like that in (2.6) to show that $ Q_n $ is an exponential recency-weighted average *without initial bias*.\n",
    "\n",
    "### Solution:\n",
    "$ \\beta_1 = 1 $ given $ \\bar{o}_0 \\doteq 0 $, and $ \\frac{\\alpha}{\\alpha} = 1 $ and converges to $ \\alpha $ as successive steps are taken. Since $ \\beta_1 = 1 $ the  reward is fully represented by the full weight and the step-size goes on to converge to the true underlying step-size $ \\alpha $ after which it behaves as constant-step size while avoiding the initial bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper-Confidence-Bound Action Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "agents = [UCBAgent(env.k, 2), EpsilonGreedyAgent(env.k, epsilon=0.1)]\n",
    "\n",
    "sim = Simulate(env, agents)\n",
    "sim.run()\n",
    "\n",
    "plot_agent_performance(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.8: Mysterious Spikes\n",
    "---\n",
    "In the above figure the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this?\n",
    "\n",
    "### Solution:\n",
    "\n",
    "The UCB algorithm is selecting actions according to:\n",
    "\n",
    "\\begin{equation*}\n",
    "A_t \\doteq \\underset{a}{\\mathrm{argmax}} \\left[ \\hspace{0.5em} Q_t(a) + c  \\sqrt{\\frac{ln(t)}{N_t(a)}} \\hspace{0.5em} \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Where c is the exploratory parameter and $N_t(a)$ is considered to be optimal if 0. Since during the first 10 timesteps there are remaining unexplored actions, they will be successively chosen and as there is only one optimal selection, the action will only be optimal in approximately 10% of the trials (as shown above with 0.1% Optimal Action selection for timestep 1-10).\n",
    "\n",
    "On the 11th step the model has a full representation of the action space and with high probability will select the optimal action from the computed action value. This is reflected by a spike where nearly 45% of the runs selected the optimal action on step 11. However, the number of selections for each action besides the one picked in step 11 is now 1, so the uncertainty for each action is still high and the suboptimal actions will be selected with decreasing probability as the number of steps increases and the confidence in the optimal actions estimation grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Bandit Algorithms\n",
    "\n",
    "Softmax Distribution\n",
    "\\begin{equation*}\n",
    "Pr \\{ A_t=a \\} \\doteq \\frac{e^{H_t(a)}}{\\sum_{b=1}^{k}e^{H_t(b)}} \\doteq \\pi_t(a)\n",
    "\\end{equation*}\n",
    "\n",
    "On each step, after selecting action $ A_t $ and receiving the reward $ R_t $,  the action preferences are updated by:\n",
    "\n",
    "\\begin{equation*}\n",
    "H_{t+1}(A_t) \\doteq H_t(A_t) + \\alpha(R_t - \\bar{R}_t)(1 - \\pi_t(A_t)) \\text{, and} \n",
    "\\\\\n",
    "H_{t+1}(a) \\doteq H_t(a) - \\alpha(R_t - \\bar{R}_t)\\pi_t(a) \\text{, for all $ a \\neq A_t$}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAgent:\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, alpha=0.1, baseline=False):\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.baseline=baseline\n",
    "        self.action_space = np.arange(self.k)\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"GradientSoftmax Agent | α = {self.alpha}, baseline = {self.baseline}\"\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.time = 0\n",
    "        self.avg_reward = 0\n",
    "        self.q_est = np.zeros(self.k)\n",
    "        self.action_count = np.zeros(self.k)\n",
    "        self.action_prob = np.ones(self.k) / self.k\n",
    "        \n",
    "    \n",
    "    def act(self):\n",
    "        return np.random.choice(self.action_space, p=self.action_prob)\n",
    "    \n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        self.time += 1\n",
    "        self.action_count[action] += 1\n",
    "        self.avg_reward += (reward - self.avg_reward) / self.time\n",
    "        \n",
    "        exp_q_est = np.exp(self.q_est)\n",
    "        self.action_prob = exp_q_est / np.sum(exp_q_est)\n",
    "        \n",
    "        one_hot = np.zeros(self.k)\n",
    "        one_hot[action] = 1\n",
    "        \n",
    "        baseline = self.avg_reward if self.baseline else 0\n",
    "        self.q_est += self.alpha * (reward - baseline) * (one_hot - self.action_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "agents = [GradientAgent(env.k, 0.25), GradientAgent(env.k, 0.25, True)]\n",
    "\n",
    "sim = Simulate(env, agents)\n",
    "sim.run()\n",
    "\n",
    "plot_agent_performance(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2.5\n",
    "---\n",
    "Average performance of the gradient bandit algorithm with and without a reward baseline on the 10-armed testbed when the $ q_*(a) $ are chosen to be near +4 rather than near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(mean=4.0)\n",
    "agents = [\n",
    "            GradientAgent(env.k, 0.1, True), \n",
    "            GradientAgent(env.k, 0.1, False), \n",
    "            GradientAgent(env.k, 0.4, True), \n",
    "            GradientAgent(env.k, 0.4, False)\n",
    "         ]\n",
    "\n",
    "sim = Simulate(env, agents)\n",
    "sim.run()\n",
    "\n",
    "plot_agent_performance(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.9\n",
    "---\n",
    "Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.\n",
    "\n",
    "### Solution\n",
    "\n",
    "The logistic function in artificial neural networks is given as:\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x) = \\frac{L}{1+e^{-k(x-x_0)}}\n",
    "\\end{equation*}\n",
    "\n",
    "Where:\n",
    "\n",
    "$ L $ = maximum value of the curve\n",
    "\n",
    "$ k $ = Steepness\n",
    "\n",
    "$ x_0 $ = X value of the midpoint.\n",
    "\n",
    "\n",
    "The sigmoid function is the standard logistic function is given as a standard logistic function by:\n",
    "\n",
    "$ L = 1 $\n",
    "\n",
    "$ k = 1 $\n",
    "\n",
    "$ x_0 = 0 $\n",
    "\n",
    "\\begin{equation*}\n",
    "S(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{equation*}\n",
    "\n",
    "In the binary case, we can see that the softmax distribution expands to:\n",
    "\n",
    "\\begin{equation*}\n",
    "Pr \\{ A_t=a \\} \\doteq \\frac{e^{H_t(a)}}{e^{H_t(a)} + e^{H_t(b)}}\n",
    "= \\frac{1}{1 + e^{H_t(b)-H_t(a)}}\n",
    "\\end{equation*}\n",
    "\n",
    "Now note that the structure of the exponent in the logistic $ -k(x-x_0) $. Observe that the exponent in the denominator  can be expressed in the same form $ (H_t(b) - H_t(a)) = -(H_t(a)-H_t(b)) $. Substituting the above into the binary softmax equation we obtain the logistic function in standard form:\n",
    "\\begin{align*}\n",
    "\\frac{1}{1 + e^{-(H_t(a)-H_t(b))}} = f(x)\n",
    "\\end{align*}\n",
    "\n",
    "And let:\n",
    "\n",
    "$ x = H_t(a) - H_t(b) $\n",
    "\n",
    "to place the equation in the form of the sigmoid:\n",
    "\\begin{align*}\n",
    "&= \\frac{1}{1 + e^{-x}} = S(x)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associative Search (Contextual Bandits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.10\n",
    "---\n",
    "Suppose you face a 2-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5 (case A), and 0.9 and 0.8 with probability 0.5 (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don't know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?\n",
    "\n",
    "### Solution\n",
    "\n",
    "In a situation where I as the agent am not able to determine the case I am facing, I know ahead of time that the probability of each case is 0.5. Generally computing the optimal action could be done by averaging the expected reward for each action across each case. However, I also know the alternating values have and can determine that in the long-run the expected reward for either action selection is equivalent $( 0.5 )$. In this case I don't know that the non-stationary techniques would hold any advantage over using the stationary methods as the optimal possible reward with no information is effectively no better than random.\n",
    "\n",
    "In the case where I am able to determine which case I am facing, I can leverage a distinct non-stationary algorithm for each case individually and selectively apply it when I observe the state prior to taking an action. With an optimal policy learned the best I could hope to achieve is selecting action 2 in Case A and action 1 in case B and to earn an average reward of $ (0.5)(0.2) + (0.5)(0.9) = 0.55 $ slightly above random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSummary(sample, parameters, labels, cFrom = 0):\n",
    "    rewardMean = np.mean(sample.rewards[:,cFrom:,:], axis=2)\n",
    "    agentsChoice = np.arange(len(labels))\n",
    "    i=0\n",
    "    for a, parameter in zip(agentsChoice, parameters):\n",
    "            l = len(parameter)\n",
    "            plt.plot(parameter, np.mean(rewardMean[i:i+l], axis=1)\n",
    "                    , markersize=0.7, lw=1.5, label=labels[a])\n",
    "            i += l\n",
    "    plt.xlabel('Parameter(2^x)')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend(loc='best', fancybox=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: 0.0\n",
      "Initial: 0.0\n",
      "Initial: 0.0\n",
      "Initial: 0.0\n",
      "Initial: 0.0\n",
      "Initial: 0.0\n",
      "Initial: 0.25\n",
      "Initial: 0.5\n",
      "Initial: 1.0\n",
      "Initial: 2.0\n",
      "Initial: 4.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978522ab609547a397b43535a521b9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d43f845d79454f984bcd7a2d3f0b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848f343bc14c40728042a116f52744a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d2dec3e281438abae82e9f413a8e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435d6f5444644b3a95d84b6997addc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00d1a08546445a2bdf9fe817455a322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e345724ede548b181380d327caf08fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc68a6a07fd41aebdb260363832bc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc33e967994b439294ebc2f45b2a5838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e163fb32654883a0b255e23c5e3795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680fde76d07b42eaad12a3f09fab631e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23267ee538614247abefa07ca71af16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7730feb3976e4147aea209d011487de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab17503146840babd212b00af738bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25189e17e4a344d38eb02bc857dac07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0b0bf55d914977a83ef1faffe87595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3412a203486c41059f4f445cb41a134c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea27ee53baae4976b6bb0614c0480ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec270abbd8c24ffc9abb043815b13b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8436e48aa1d3442f9a2e1fba99aa7431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1412141f51f4d82a7dc48396e012aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69eb123b35f84ad8ba3710cd083133f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eca3a6cf1984297aef99aad81c128df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6c675262db4ae19fcae9b4ac94cf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae1007c5d484559a9a4bb63a75a07b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9fa0fee80aaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mplotSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parameters' is not defined"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "labels = ['epsilon-greedy', 'gradient bandit', 'UCB', 'optimistic initialization']\n",
    "generators = [\n",
    "                lambda epsilon: EpsilonGreedyAgent(env.k, epsilon=epsilon),\n",
    "                lambda initial: EpsilonGreedyAgent(env.k, epsilon=0, alpha=0.1, initial=initial),\n",
    "                lambda c: UCBAgent(env.k, c=c),\n",
    "                lambda alpha: GradientAgent(env.k, alpha=alpha, baseline=True)\n",
    "]\n",
    "params = [\n",
    "    np.arange(-7, -1, dtype=np.float),\n",
    "    np.arange(-2, 3, dtype=np.float),\n",
    "    np.arange(-4, 3, dtype=np.float),\n",
    "    np.arange(-5, 2, dtype=np.float)\n",
    "]\n",
    "agents = []\n",
    "\n",
    "for generator, parameter in zip(generators, params):\n",
    "    for param in parameter:\n",
    "        agents.append(generator(pow(2, param)))\n",
    "        \n",
    "sim = Simulate(env, agents, runs=2000, steps=1000)\n",
    "sim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Simulate' object has no attribute 'rewardData'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-63b85511df8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplotSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-b853fe97dcc0>\u001b[0m in \u001b[0;36mplotSummary\u001b[0;34m(sample, parameters, labels, cFrom)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplotSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcFrom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrewardMean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewardData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcFrom\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0magentsChoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magentsChoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Simulate' object has no attribute 'rewardData'"
     ]
    }
   ],
   "source": [
    "plotSummary(sim, params, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.11\n",
    "---\n",
    "Make a figure analogous to Figure 2.6 for the nonstationary case outlined in Exercise 2.5. Include the constant-step-size $ \\epsilon $-greedy algorithm with $ \\alpha=0.1 $. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
