{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tnrange\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agent-Environment Interface\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "---\n",
    "Device three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n",
    "\n",
    "### Solution:\n",
    "---\n",
    "\n",
    "#### 1) Intelligent Trade Execution Engine\n",
    "\n",
    "Actions: Buy, Sell, Price, Qty\n",
    "\n",
    "States: Order Book, Metric Signals\n",
    "\n",
    "Rewards: Profit/Loss\n",
    "\n",
    "#### 2) Tabletop Game Agent\n",
    "\n",
    "Actions: Plays and actions available to the player\n",
    "\n",
    "States: Board state, hand, other players, available resources\n",
    "\n",
    "Rewards: Score, Resources, Area Controlled\n",
    "\n",
    "#### 3) Autopilot\n",
    "Actions: Throttle, Flaps, Up, Down\n",
    "\n",
    "States: Altitude Sensors, \n",
    "\n",
    "Rewards: Reward for time spent in tolerance of target altitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2\n",
    "---\n",
    "Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "### Solution:\n",
    "---\n",
    "The Markov Decision Process is limited by the Markov Property, in that the probabilities of each possible state depends only upon the immediately preceding state and action, and not at all on earlier states and actions. Any goal-directly learning task that violated the Markov property would not fit within the framework. For example, a game where previous moves are hidden from the observer but impact the state of the game. A blind auction for instance.\n",
    "\n",
    "Another limitation is that any process modeled by MDP must be representationally finite. So if we come up with a continuous task with a state/action space that is intractably large then it cannot be suitably addressed by the Markov Decision framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3\n",
    "---\n",
    "Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out -- say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in -- say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?\n",
    "\n",
    "\n",
    "### Solution:\n",
    "---\n",
    "I don't know that there would be a single right level at which to define the interface but it does seem there would be some rank order preference. The interface should be defined at a suitably \"high\" level such as to reduce the state space complexity yet low enough to enact influence on actions that have strong impact on completing goals for rewards. The measure of one being preferred over another would quantitatively be reliable performance of the models based on the agent boundary relative to other methods and convergence properties. When selecting the interface it should be chosen such that the state space is tractable yet the agent is still provided with action variability to execute on micro-tasks required to reach a goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4\n",
    "---\n",
    "Give a table analogous to that in Example 3.3, but for $ p(s', r|s, a) $. It should have columns for $ s, a, s', r, $ and $ p(s',r|s,a) $, and a row for every 4-tuple for which $ p(s',r|s,a) > 0 $\n",
    "\n",
    "### Solution:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>a</th>\n",
       "      <th>s'</th>\n",
       "      <th>r</th>\n",
       "      <th>p(s',r|s,a)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high</td>\n",
       "      <td>search</td>\n",
       "      <td>high</td>\n",
       "      <td>r_search</td>\n",
       "      <td>α</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>search</td>\n",
       "      <td>low</td>\n",
       "      <td>r_search</td>\n",
       "      <td>1-α</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low</td>\n",
       "      <td>search</td>\n",
       "      <td>high</td>\n",
       "      <td>-3</td>\n",
       "      <td>1-β</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low</td>\n",
       "      <td>search</td>\n",
       "      <td>low</td>\n",
       "      <td>r_search</td>\n",
       "      <td>β</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>wait</td>\n",
       "      <td>high</td>\n",
       "      <td>1</td>\n",
       "      <td>r_wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>low</td>\n",
       "      <td>wait</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>r_wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>low</td>\n",
       "      <td>recharge</td>\n",
       "      <td>high</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      s         a    s'         r p(s',r|s,a)\n",
       "0  high    search  high  r_search           α\n",
       "1  high    search   low  r_search         1-α\n",
       "2   low    search  high        -3         1-β\n",
       "3   low    search   low  r_search           β\n",
       "4  high      wait  high         1      r_wait\n",
       "5   low      wait   low         1      r_wait\n",
       "6   low  recharge  high         1           0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    [\"high\", \"search\",\"high\",\"r_search\",\"α\"],\n",
    "    [\"high\", \"search\",\"low\",\"r_search\",\"1-α\"],\n",
    "    [\"low\", \"search\",\"high\",\"-3\",\"1-β\"],\n",
    "    [\"low\", \"search\",\"low\",\"r_search\",\"β\"],\n",
    "    [\"high\", \"wait\",\"high\",\"1\",\"r_wait\"],\n",
    "    [\"low\", \"wait\",\"low\",\"1\",\"r_wait\"],\n",
    "    [\"low\", \"recharge\",\"high\",\"1\",\"0\"]\n",
    "]\n",
    "pd.DataFrame(data, columns=[\"s\", \"a\", \"s'\", \"r\", \"p(s',r|s,a)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Returns and Episodes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5\n",
    "---\n",
    "The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that yhou know the modifications needed by giving the modified version of (3.3).\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{s' \\in S}\\sum_{r \\in R}p(s',r|s,a)=1, \\forall s \\in S, \\alpha \\in A(s) \\tag{3.3}\n",
    "\\end{equation*}\n",
    "\n",
    "### Solution:\n",
    "---\n",
    "The distinction between the episodic and continuous case is the inclusion of the terminal state. The set $ S $ used in 3.3 above denotes the set of all non-terminal states, whereas the set $ S^+ $ denotes the set of all states including terminals. Thus, the modification to (3.3) to support episodic tasks is the iteration over $ S^+ $ shown below\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{s' \\in S}\\sum_{r \\in R}p(s',r|s,a)=1, \\forall s \\in S^+, \\alpha \\in A(s)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.6\n",
    "---\n",
    "Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing fomulation of this task?\n",
    "\n",
    "### Solution:\n",
    "---\n",
    "Discretizing the above pole-balancing as episodes results in the formula that fails at terminal step K.\n",
    "\n",
    "\\begin{equation*}\n",
    "G_{t}=\\sum_{k=0}^{T-t-1}\\gamma^{k}R_{t+k+1}\n",
    "\\end{equation*}\n",
    "\n",
    "However, given that all rewards except the final reward of -1 is 0, we can remove the sum and evaluate the terminal case such that when $ k = T - t - 1 $ then  $ R_{t+k+1} = R_{t + (T - t - 1) + 1} = R_{T} = -1 $\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "G_{t}=-\\gamma^{T}\n",
    "\\end{equation*}\n",
    "\n",
    "Where in the limit the terminal episodic state $ T $ is equivalent to $ K $ being the number of steps until failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
